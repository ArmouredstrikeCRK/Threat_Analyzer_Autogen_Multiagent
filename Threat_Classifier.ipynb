{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.2.5)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/chetankothari/Library/Python/3.11/lib/python/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/chetankothari/Library/Python/3.11/lib/python/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (494 bytes)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading torch-2.7.0-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Downloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-macosx_11_0_arm64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, tqdm, sympy, safetensors, regex, pyyaml, networkx, MarkupSafe, hf-xet, fsspec, filelock, jinja2, huggingface-hub, torch, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 hf-xet-1.1.0 huggingface-hub-0.31.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path:  /Users/chetankothari/VS Projects/Threat_Analyzer_Autogen_Multiagent/Data\n",
      "Validation Images Path:  /Users/chetankothari/VS Projects/Threat_Analyzer_Autogen_Multiagent/Data/Validation Images\n",
      "Frames from Video Path:  /Users/chetankothari/VS Projects/Threat_Analyzer_Autogen_Multiagent/Data/Frames from Video\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dir=os.getcwd()\n",
    "\n",
    "#Lets set the path to the data\n",
    "data_path = os.path.join(dir, 'Data')\n",
    "#Lets set the path to the model\n",
    "calssification_model_path = os.path.join(dir, 'classification_models')\n",
    "#Lets check if this exists and if not then create it'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "if not os.path.exists(calssification_model_path):\n",
    "    os.makedirs(calssification_model_path)\n",
    "#Lets set the path for the validation images\n",
    "validation_images_path = os.path.join(data_path, 'Validation Images')\n",
    "#Lets set the path for the Frames from Video\n",
    "frames_from_video_path = os.path.join(data_path, 'Frames from Video')\n",
    "\n",
    "#Lets print the paths\n",
    "print(\"Data Path: \", data_path)\n",
    "print(\"Validation Images Path: \", validation_images_path)\n",
    "print(\"Frames from Video Path: \", frames_from_video_path)\n",
    "\n",
    "\n",
    "threat_analysis_path = os.path.join(dir, 'threat_analysis.xlsx')\n",
    "# lets set the responses path\n",
    "response_path = os.path.join(data_path, 'Responses')\n",
    "if not os.path.exists(response_path):\n",
    "    print(\"Responses not found at: \", response_path)\n",
    "#Lets set the images path\n",
    "images_path = os.path.join(data_path, 'Images')\n",
    "if not os.path.exists(images_path):\n",
    "    print(\"Images not found at: \", images_path)\n",
    "#Set the api URL\n",
    "URL = \"http://127.0.0.1:8000/analyze_image_agent\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet Names:  dict_keys(['Sheet1'])\n",
      "               Image Name                                        Description  \\\n",
      "0   coba_train (3267).jpg  Description:\\n**Scenario Overview**\\n\\nThe ima...   \n",
      "1  coba_train_bg (47).jpg  Description:\\n**Scenario Assessment**\\n\\n*   *...   \n",
      "2          Non_Threat.jpg  Description:\\n**Military Scenario Assessment**...   \n",
      "3   coba_train (2677).jpg  Description:\\n**Military Scenario Assessment**...   \n",
      "4   coba_train (4473).jpg  Description:\\n**Scene Type:** Battlefield\\n\\n*...   \n",
      "\n",
      "                                             Objects  \\\n",
      "0  Objects Detected:\\nThe image presents a collec...   \n",
      "1  Objects Detected:\\nThe image depicts a small, ...   \n",
      "2  Objects Detected:\\nThe image shows a large gre...   \n",
      "3  Objects Detected:\\nThe image depicts a cannon ...   \n",
      "4  Objects Detected:\\nThe image depicts a person ...   \n",
      "\n",
      "                                           Reasoning       Threat Class  \n",
      "0  Reasoning:\\nOkay, so I'm trying to figure out ...  Threat Level: 0\\n  \n",
      "1  Reasoning:\\nOkay, so I'm trying to figure out ...  Threat Level: 0\\n  \n",
      "2  Reasoning:\\nOkay, so I'm trying to figure out ...  Threat Level: 0\\n  \n",
      "3  Reasoning:\\nOkay, so I need to figure out the ...  Threat Level: 0\\n  \n",
      "4  Reasoning:\\nOkay, so I need to figure out the ...  Threat Level: 3\\n  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Lets load the threat analysis data to dataframe\n",
    "threat_analysis_df = pd.read_excel(threat_analysis_path, sheet_name=None)\n",
    "#Lets get the sheet names\n",
    "sheet_names = threat_analysis_df.keys()\n",
    "#Lets print the sheet names\n",
    "print(\"Sheet Names: \", sheet_names)\n",
    "#Lets get the first sheet\n",
    "sheet_names_list = list(sheet_names)  # Convert dict_keys to a list\n",
    "threat_analysis_df = threat_analysis_df[sheet_names_list[0]]\n",
    "#Lets print the first 5 rows\n",
    "print(threat_analysis_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image Name                                        Description  \\\n",
      "0   coba_train (3267)  Description:\\n**Scenario Overview**\\n\\nThe ima...   \n",
      "1  coba_train_bg (47)  Description:\\n**Scenario Assessment**\\n\\n*   *...   \n",
      "2          Non_Threat  Description:\\n**Military Scenario Assessment**...   \n",
      "3   coba_train (2677)  Description:\\n**Military Scenario Assessment**...   \n",
      "4   coba_train (4473)  Description:\\n**Scene Type:** Battlefield\\n\\n*...   \n",
      "\n",
      "   Threat Class  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             3  \n",
      "<bound method DataFrame.__len__ of             Image Name                                        Description  \\\n",
      "0    coba_train (3267)  Description:\\n**Scenario Overview**\\n\\nThe ima...   \n",
      "1   coba_train_bg (47)  Description:\\n**Scenario Assessment**\\n\\n*   *...   \n",
      "2           Non_Threat  Description:\\n**Military Scenario Assessment**...   \n",
      "3    coba_train (2677)  Description:\\n**Military Scenario Assessment**...   \n",
      "4    coba_train (4473)  Description:\\n**Scene Type:** Battlefield\\n\\n*...   \n",
      "..                 ...                                                ...   \n",
      "79   coba_train (2176)  Description:\\n**Scenario Assessment**\\n\\n*   *...   \n",
      "80    coba_train (972)  Description:\\n**Military Scenario Assessment**...   \n",
      "81   coba_train (1776)  Description:\\n**Assessment:**\\n\\n*   **Scene T...   \n",
      "82   coba_train (5298)  Description:\\n**Military Scenario Assessment**...   \n",
      "83   coba_train (1672)  Description:\\n**Military Scenario Assessment**...   \n",
      "\n",
      "    Threat Class  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              3  \n",
      "..           ...  \n",
      "79             0  \n",
      "80             1  \n",
      "81             0  \n",
      "82             0  \n",
      "83             2  \n",
      "\n",
      "[84 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "df_temp = threat_analysis_df.copy()\n",
    "# Drop the file extension from the image names\n",
    "df_temp['Image Name'] = df_temp['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "# Now lets drop the unwanted text from column threat class\n",
    "df_temp['Threat Class'] = df_temp['Threat Class'].str.extract(r'(\\d+)').astype(int)\n",
    "# Lets drop the unwanted columns    \n",
    "df_temp.drop(columns=['Objects', 'Reasoning'], inplace=True)\n",
    "print(df_temp.head())\n",
    "print(df_temp.__len__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Image Name                                        Description  \\\n",
      "0   coba_train (3267)  Description:\\n**Scenario Overview**\\n\\nThe ima...   \n",
      "1  coba_train_bg (47)  Description:\\n**Scenario Assessment**\\n\\n*   *...   \n",
      "2          Non_Threat  Description:\\n**Military Scenario Assessment**...   \n",
      "3   coba_train (2677)  Description:\\n**Military Scenario Assessment**...   \n",
      "4   coba_train (4473)  Description:\\n**Scene Type:** Battlefield\\n\\n*...   \n",
      "\n",
      "   Threat Class  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             3  \n",
      "<bound method DataFrame.__len__ of             Image Name                                        Description  \\\n",
      "0    coba_train (3267)  Description:\\n**Scenario Overview**\\n\\nThe ima...   \n",
      "1   coba_train_bg (47)  Description:\\n**Scenario Assessment**\\n\\n*   *...   \n",
      "2           Non_Threat  Description:\\n**Military Scenario Assessment**...   \n",
      "3    coba_train (2677)  Description:\\n**Military Scenario Assessment**...   \n",
      "4    coba_train (4473)  Description:\\n**Scene Type:** Battlefield\\n\\n*...   \n",
      "..                 ...                                                ...   \n",
      "79   coba_train (2176)  Description:\\n**Scenario Assessment**\\n\\n*   *...   \n",
      "80    coba_train (972)  Description:\\n**Military Scenario Assessment**...   \n",
      "81   coba_train (1776)  Description:\\n**Assessment:**\\n\\n*   **Scene T...   \n",
      "82   coba_train (5298)  Description:\\n**Military Scenario Assessment**...   \n",
      "83   coba_train (1672)  Description:\\n**Military Scenario Assessment**...   \n",
      "\n",
      "    Threat Class  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              3  \n",
      "..           ...  \n",
      "79             0  \n",
      "80             1  \n",
      "81             0  \n",
      "82             0  \n",
      "83             2  \n",
      "\n",
      "[84 rows x 3 columns]>\n",
      "Index(['Image Name', 'Description', 'Threat Class'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Lets copy the dataframe back the threat_analysis_df\n",
    "threat_analysis_df = df_temp.copy()\n",
    "print(threat_analysis_df.head())\n",
    "print(threat_analysis_df.__len__)\n",
    "print(threat_analysis_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Files:  ['coba_train (5454).json', 'coba_train (1402).json', 'coba_train (3184).json', 'coba_train (3510).json', 'coba_train (5344).json', 'coba_train (4795).json', 'coba_train (4867).json', 'coba_train (1638).json', 'Non_Threat.json', 'coba_train (5413).json', 'coba_train (972).json', 'coba_train (2182).json', 'coba_train (2984).json', 'coba_train (4307).json', 'coba_train (1015).json', 'coba_train (4958).json', 'coba_train (5452).json', 'coba_train_bg (34).json', 'coba_train (3267).json', 'coba_train (194).json', 'coba_train (2575).json', 'coba_train (1603).json', 'coba_train (3735).json', 'coba_train_bg (12).json', 'coba_train (418).json', 'coba_train (5692).json', 'coba_train (4473).json', 'coba_train (3102).json', 'coba_train (1559).json', 'coba_train (2187).json', 'coba_train (1479).json', 'coba_train (5468).json', 'coba_train (5298).json', 'coba_train (4548).json', 'coba_train (3594).json', 'coba_train (3695).json', 'coba_train_bg (33).json', 'coba_train (4434).json', 'coba_train (2609).json', 'coba_train (1193).json', 'coba_train (4021).json', 'coba_train (4383).json', 'coba_train (3437).json', 'coba_train (3183).json', 'coba_train (2677).json', 'coba_train (4910).json', 'coba_train (5995).json', 'coba_train (821).json', 'coba_train (2799).json', 'coba_train (633).json', 'coba_train (5405).json', 'coba_train (2680).json', 'coba_train_bg (47).json', 'coba_train (1672).json', 'coba_train (5051).json', 'coba_train (2817).json', 'coba_train (1146).json', 'coba_train (1776).json', 'coba_train (2176).json', 'coba_train (1789).json', 'coba_train (1448).json', 'coba_train (6000).json', 'coba_train (2831).json', 'coba_train (3543).json', 'coba_train (4640).json', 'coba_train (1183).json', 'coba_train (5275).json', 'coba_train (3722).json', 'Midium_Threat.json', 'coba_train (4487).json', 'coba_train (4607).json', 'coba_train (5889).json', 'Test_Image_Semi_Threat.json', 'coba_train (4928).json', 'coba_train (474).json', 'coba_train (1634).json', 'coba_train (5662).json', 'coba_train (340).json', 'coba_train (3719).json', 'coba_train (1476).json', 'coba_train_bg (27).json', 'coba_train (5601).json', 'coba_train (3816).json', 'coba_train (3853).json']\n",
      "            Image Name                                    Vision_Analysis  \\\n",
      "0    coba_train (3267)  {'AGGRESSION_ANALYSIS': '**AGGRESSION_ANALYSIS...   \n",
      "1   coba_train_bg (47)  {'AGGRESSION_ANALYSIS': '**AGGRESSION ANALYSIS...   \n",
      "2           Non_Threat  {'AGGRESSION_ANALYSIS': '**AGGRESSION ANALYSIS...   \n",
      "3    coba_train (2677)  {'AGGRESSION_ANALYSIS': '**AGGRESSION ANALYSIS...   \n",
      "4    coba_train (4473)  {'AGGRESSION_ANALYSIS': '**Aggression Analysis...   \n",
      "..                 ...                                                ...   \n",
      "79   coba_train (2176)  {'AGGRESSION_ANALYSIS': '**AGGRESSION ANALYSIS...   \n",
      "80    coba_train (972)  {'AGGRESSION_ANALYSIS': '**AGGRESSION ANALYSIS...   \n",
      "81   coba_train (1776)  {'AGGRESSION_ANALYSIS': '**AGGRESSION ANALYSIS...   \n",
      "82   coba_train (5298)  {'AGGRESSION_ANALYSIS': 'Based on the image, h...   \n",
      "83   coba_train (1672)  {'AGGRESSION_ANALYSIS': '**AGGRESSION_ANALYSIS...   \n",
      "\n",
      "   Threat Level  \n",
      "0             1  \n",
      "1             1  \n",
      "2             0  \n",
      "3             0  \n",
      "4             2  \n",
      "..          ...  \n",
      "79            0  \n",
      "80            0  \n",
      "81            0  \n",
      "82            1  \n",
      "83            0  \n",
      "\n",
      "[84 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Now lets prepare another data from from responses of multiagent analysis\n",
    "#Lets load the threat analysis data to dataframe the responses are set of json files which we would read to create a dataframe with each record being samed with the image name\n",
    "#Lets read the response files\n",
    "image_files = os.listdir(images_path)\n",
    "#Lets set the response files path\n",
    "response_files = os.listdir(response_path)\n",
    "print(\"Response Files: \", response_files)\n",
    "#Lets create a empty dataframe\n",
    "response_df = pd.DataFrame()\n",
    "#Lets loop through the response files\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "# Loop through each file in the folder\n",
    "for image_file in image_files:\n",
    "    response_file = os.path.join(response_path, os.path.splitext(image_file)[0] + '.json')\n",
    "    if os.path.exists(response_file):\n",
    "        with open(response_file, 'r', encoding='utf-8') as f:\n",
    "            response_data = json.load(f)\n",
    "        #Extract the Vision Analysis data\n",
    "        vision_analysis = response_data.get('vision_analysis', {})\n",
    "        # Extract and handle the threat level\n",
    "        threat_level = response_data.get('threat_level', 'N/A')\n",
    "        if isinstance(threat_level, str):\n",
    "            # Handle cases where threat_level is a string\n",
    "            numeric_threat_level = threat_level.split(\"–\")[0].strip()  # Extract the numeric part\n",
    "        elif isinstance(threat_level, dict):\n",
    "            # Handle cases where threat_level is a dictionary\n",
    "            numeric_threat_level = threat_level.get('threatLevel', 'N/A')\n",
    "        else:\n",
    "            # Handle unexpected cases\n",
    "            numeric_threat_level = 'N/A'\n",
    "\n",
    "        # Append the image name and numeric threat level to the data list\n",
    "        data.append({\"Image Name\": image_file,\"Vision_Analysis\":vision_analysis,\"Threat Level\": numeric_threat_level})\n",
    "# Create a DataFrame from the collected data\n",
    "multiagent_threat_df = pd.DataFrame(data)\n",
    "# Drop the file extension from the image names \n",
    "multiagent_threat_df['Image Name'] = multiagent_threat_df['Image Name'].str.replace('.jpg', '', regex=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(multiagent_threat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Image Name', 'Vision_Analysis', 'Threat Level'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(multiagent_threat_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/61/39zk8n_x5l32mb7rjkls_4n00000gn/T/ipykernel_10931/2989730700.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  X = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "#Prepare the data for training\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to convert text to BERT embeddings\n",
    "def extract_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Mean pooling of token embeddings\n",
    "\n",
    "# Convert all descriptions to BERT embeddings\n",
    "import json\n",
    "\n",
    "# Ensure all entries in the Vision_Analysis column are strings\n",
    "multiagent_threat_df[\"Vision_Analysis\"] = multiagent_threat_df[\"Vision_Analysis\"].apply(\n",
    "    lambda x: json.dumps(x) if isinstance(x, dict) else str(x)\n",
    ")\n",
    "\n",
    "# Convert all descriptions to BERT embeddings\n",
    "X = torch.tensor(\n",
    "    [np.array(extract_bert_embeddings(desc), dtype=np.float32) for desc in multiagent_threat_df[\"Vision_Analysis\"]]\n",
    ")\n",
    "# Ensure all values in the \"Threat Level\" column are integers\n",
    "multiagent_threat_df[\"Threat Level\"] = pd.to_numeric(multiagent_threat_df[\"Threat Level\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Convert threat levels to numerical values\n",
    "y = torch.tensor(multiagent_threat_df[\"Threat Level\"].values)  # Labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)  # Final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Model hyperparameters\n",
    "input_dim = 768  # BERT embedding size\n",
    "hidden_dim = 2096  # Hidden layer size\n",
    "output_dim = 6  # Number of threat classes (0-5)\n",
    "\n",
    "# Initialize model, loss function, optimizer\n",
    "model = MLPClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 13.1788\n",
      "Epoch 2/100, Loss: 10.0495\n",
      "Epoch 3/100, Loss: 9.6183\n",
      "Epoch 4/100, Loss: 9.5567\n",
      "Epoch 5/100, Loss: 9.3507\n",
      "Epoch 6/100, Loss: 9.1926\n",
      "Epoch 7/100, Loss: 9.0519\n",
      "Epoch 8/100, Loss: 8.8713\n",
      "Epoch 9/100, Loss: 8.6890\n",
      "Epoch 10/100, Loss: 8.4819\n",
      "Epoch 11/100, Loss: 8.2464\n",
      "Epoch 12/100, Loss: 7.9954\n",
      "Epoch 13/100, Loss: 7.7197\n",
      "Epoch 14/100, Loss: 7.4275\n",
      "Epoch 15/100, Loss: 7.1225\n",
      "Epoch 16/100, Loss: 6.8069\n",
      "Epoch 17/100, Loss: 6.4881\n",
      "Epoch 18/100, Loss: 6.1774\n",
      "Epoch 19/100, Loss: 5.8664\n",
      "Epoch 20/100, Loss: 5.5703\n",
      "Epoch 21/100, Loss: 5.2991\n",
      "Epoch 22/100, Loss: 5.0403\n",
      "Epoch 23/100, Loss: 4.7928\n",
      "Epoch 24/100, Loss: 4.5650\n",
      "Epoch 25/100, Loss: 4.3528\n",
      "Epoch 26/100, Loss: 4.1521\n",
      "Epoch 27/100, Loss: 3.9612\n",
      "Epoch 28/100, Loss: 3.7724\n",
      "Epoch 29/100, Loss: 3.5913\n",
      "Epoch 30/100, Loss: 3.4158\n",
      "Epoch 31/100, Loss: 3.2604\n",
      "Epoch 32/100, Loss: 3.1017\n",
      "Epoch 33/100, Loss: 2.9494\n",
      "Epoch 34/100, Loss: 2.8067\n",
      "Epoch 35/100, Loss: 2.6608\n",
      "Epoch 36/100, Loss: 2.5190\n",
      "Epoch 37/100, Loss: 2.3825\n",
      "Epoch 38/100, Loss: 2.2565\n",
      "Epoch 39/100, Loss: 2.1344\n",
      "Epoch 40/100, Loss: 2.0190\n",
      "Epoch 41/100, Loss: 1.9086\n",
      "Epoch 42/100, Loss: 1.7986\n",
      "Epoch 43/100, Loss: 1.7003\n",
      "Epoch 44/100, Loss: 1.6014\n",
      "Epoch 45/100, Loss: 1.5065\n",
      "Epoch 46/100, Loss: 1.4093\n",
      "Epoch 47/100, Loss: 1.3178\n",
      "Epoch 48/100, Loss: 1.2360\n",
      "Epoch 49/100, Loss: 1.1563\n",
      "Epoch 50/100, Loss: 1.0960\n",
      "Epoch 51/100, Loss: 1.0344\n",
      "Epoch 52/100, Loss: 0.9811\n",
      "Epoch 53/100, Loss: 0.9358\n",
      "Epoch 54/100, Loss: 0.8890\n",
      "Epoch 55/100, Loss: 0.8427\n",
      "Epoch 56/100, Loss: 0.7961\n",
      "Epoch 57/100, Loss: 0.7480\n",
      "Epoch 58/100, Loss: 0.7085\n",
      "Epoch 59/100, Loss: 0.6707\n",
      "Epoch 60/100, Loss: 0.6407\n",
      "Epoch 61/100, Loss: 0.6112\n",
      "Epoch 62/100, Loss: 0.5810\n",
      "Epoch 63/100, Loss: 0.5506\n",
      "Epoch 64/100, Loss: 0.5213\n",
      "Epoch 65/100, Loss: 0.4970\n",
      "Epoch 66/100, Loss: 0.4720\n",
      "Epoch 67/100, Loss: 0.4522\n",
      "Epoch 68/100, Loss: 0.4272\n",
      "Epoch 69/100, Loss: 0.4065\n",
      "Epoch 70/100, Loss: 0.3879\n",
      "Epoch 71/100, Loss: 0.3697\n",
      "Epoch 72/100, Loss: 0.3516\n",
      "Epoch 73/100, Loss: 0.3348\n",
      "Epoch 74/100, Loss: 0.3189\n",
      "Epoch 75/100, Loss: 0.3037\n",
      "Epoch 76/100, Loss: 0.2903\n",
      "Epoch 77/100, Loss: 0.2760\n",
      "Epoch 78/100, Loss: 0.2642\n",
      "Epoch 79/100, Loss: 0.2513\n",
      "Epoch 80/100, Loss: 0.2409\n",
      "Epoch 81/100, Loss: 0.2290\n",
      "Epoch 82/100, Loss: 0.2201\n",
      "Epoch 83/100, Loss: 0.2091\n",
      "Epoch 84/100, Loss: 0.2011\n",
      "Epoch 85/100, Loss: 0.1918\n",
      "Epoch 86/100, Loss: 0.1836\n",
      "Epoch 87/100, Loss: 0.1759\n",
      "Epoch 88/100, Loss: 0.1682\n",
      "Epoch 89/100, Loss: 0.1617\n",
      "Epoch 90/100, Loss: 0.1548\n",
      "Epoch 91/100, Loss: 0.1487\n",
      "Epoch 92/100, Loss: 0.1424\n",
      "Epoch 93/100, Loss: 0.1372\n",
      "Epoch 94/100, Loss: 0.1316\n",
      "Epoch 95/100, Loss: 0.1264\n",
      "Epoch 96/100, Loss: 0.1217\n",
      "Epoch 97/100, Loss: 0.1169\n",
      "Epoch 98/100, Loss: 0.1126\n",
      "Epoch 99/100, Loss: 0.1084\n",
      "Epoch 100/100, Loss: 0.1045\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X.float())\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.float()).argmax(dim=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Make sure this is the BERT model, not the MLP model\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Mean pooling of token embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT separately to avoid overwriting\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def extract_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)  # Use bert_model instead of model\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Threat Level: 4\n"
     ]
    }
   ],
   "source": [
    "new_text = \"An armed individual is moving near the perimeter.\"\n",
    "# Extract BERT embeddings for the new text\n",
    "new_embedding = extract_bert_embeddings(new_text)  # Already returns a numpy array\n",
    "new_embedding = torch.tensor(new_embedding).float().unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "\n",
    "# Pass the embeddings to the MLPClassifier\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_threat = model(new_embedding).argmax(dim=1).item()\n",
    "\n",
    "print(f\"Predicted Threat Level: {predicted_threat}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]   videoframe_0.png → videoframe_0.json\n",
      "[OK]   videoframe_52051.png → videoframe_52051.json\n",
      "[OK]   Test_new.png → Test_new.json\n",
      "[OK]   videoframe_6565.png → videoframe_6565.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "validation_response_path = os.path.join(data_path, 'validation_responses')\n",
    "if not os.path.exists(validation_response_path):\n",
    "    os.makedirs(validation_response_path)\n",
    "exts = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')\n",
    "for fname in os.listdir(frames_from_video_path):\n",
    "    if not fname.lower().endswith(exts):\n",
    "        continue\n",
    "    \n",
    "    in_path  = os.path.join(frames_from_video_path, fname)\n",
    "    out_name = os.path.splitext(fname)[0] + '.json'\n",
    "    out_path = os.path.join(validation_response_path, out_name)\n",
    "    \n",
    "    with open(in_path, 'rb') as img_file:\n",
    "        # service expects the image under \"file\"\n",
    "        files = {'file': (fname, img_file, 'application/octet-stream')}\n",
    "        try:\n",
    "            resp = requests.post(URL, files=files, data={'request_type': 'vision_analysis'})\n",
    "            resp.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[ERROR] {fname}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # write JSON (or raw text on parse failure)\n",
    "    try:\n",
    "        data = resp.json()\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    except ValueError:\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(resp.text)\n",
    "    \n",
    "    print(f\"[OK]   {fname} → {out_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [17:50:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [17:50:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [17:50:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [17:50:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [17:50:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [17:50:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 — Loss: 0.8335\n",
      "Epoch 20/200 — Loss: 0.8823\n",
      "Epoch 30/200 — Loss: 0.7391\n",
      "Epoch 40/200 — Loss: 0.7582\n",
      "Epoch 50/200 — Loss: 0.7194\n",
      "Epoch 60/200 — Loss: 0.4418\n",
      "Epoch 70/200 — Loss: 0.4034\n",
      "Epoch 80/200 — Loss: 0.2985\n",
      "Epoch 90/200 — Loss: 0.3836\n",
      "Epoch 100/200 — Loss: 0.4121\n",
      "Epoch 110/200 — Loss: 0.2779\n",
      "Epoch 120/200 — Loss: 0.2513\n",
      "Epoch 130/200 — Loss: 0.1363\n",
      "Epoch 140/200 — Loss: 0.1092\n",
      "Epoch 150/200 — Loss: 0.1110\n",
      "Epoch 160/200 — Loss: 0.0670\n",
      "Epoch 170/200 — Loss: 0.1266\n",
      "Epoch 180/200 — Loss: 0.0514\n",
      "Epoch 190/200 — Loss: 0.0465\n",
      "Epoch 200/200 — Loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_983aa\">\n",
       "  <caption>Comparison of All Models (GBT & CPU-XGB + MLP)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_983aa_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_983aa_level0_col1\" class=\"col_heading level0 col1\" >CV F1 Mean</th>\n",
       "      <th id=\"T_983aa_level0_col2\" class=\"col_heading level0 col2\" >CV F1 Std</th>\n",
       "      <th id=\"T_983aa_level0_col3\" class=\"col_heading level0 col3\" >Test Accuracy</th>\n",
       "      <th id=\"T_983aa_level0_col4\" class=\"col_heading level0 col4\" >Test Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_983aa_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_983aa_row0_col0\" class=\"data row0 col0\" >SGD_Logistic</td>\n",
       "      <td id=\"T_983aa_row0_col1\" class=\"data row0 col1\" >0.463600</td>\n",
       "      <td id=\"T_983aa_row0_col2\" class=\"data row0 col2\" >0.236500</td>\n",
       "      <td id=\"T_983aa_row0_col3\" class=\"data row0 col3\" >0.411800</td>\n",
       "      <td id=\"T_983aa_row0_col4\" class=\"data row0 col4\" >0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983aa_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_983aa_row1_col0\" class=\"data row1 col0\" >RandomForest</td>\n",
       "      <td id=\"T_983aa_row1_col1\" class=\"data row1 col1\" >0.488800</td>\n",
       "      <td id=\"T_983aa_row1_col2\" class=\"data row1 col2\" >0.106000</td>\n",
       "      <td id=\"T_983aa_row1_col3\" class=\"data row1 col3\" >0.588200</td>\n",
       "      <td id=\"T_983aa_row1_col4\" class=\"data row1 col4\" >0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983aa_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_983aa_row2_col0\" class=\"data row2 col0\" >GradientBoosting</td>\n",
       "      <td id=\"T_983aa_row2_col1\" class=\"data row2 col1\" >0.544500</td>\n",
       "      <td id=\"T_983aa_row2_col2\" class=\"data row2 col2\" >0.070900</td>\n",
       "      <td id=\"T_983aa_row2_col3\" class=\"data row2 col3\" >0.705900</td>\n",
       "      <td id=\"T_983aa_row2_col4\" class=\"data row2 col4\" >0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983aa_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_983aa_row3_col0\" class=\"data row3 col0\" >XGB_CPU</td>\n",
       "      <td id=\"T_983aa_row3_col1\" class=\"data row3 col1\" >0.561600</td>\n",
       "      <td id=\"T_983aa_row3_col2\" class=\"data row3 col2\" >0.114500</td>\n",
       "      <td id=\"T_983aa_row3_col3\" class=\"data row3 col3\" >0.647100</td>\n",
       "      <td id=\"T_983aa_row3_col4\" class=\"data row3 col4\" >0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983aa_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_983aa_row4_col0\" class=\"data row4 col0\" >LinearSVM</td>\n",
       "      <td id=\"T_983aa_row4_col1\" class=\"data row4 col1\" >0.519100</td>\n",
       "      <td id=\"T_983aa_row4_col2\" class=\"data row4 col2\" >0.037600</td>\n",
       "      <td id=\"T_983aa_row4_col3\" class=\"data row4 col3\" >0.529400</td>\n",
       "      <td id=\"T_983aa_row4_col4\" class=\"data row4 col4\" >0.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983aa_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_983aa_row5_col0\" class=\"data row5 col0\" >MLPClassifier</td>\n",
       "      <td id=\"T_983aa_row5_col1\" class=\"data row5 col1\" >N/A</td>\n",
       "      <td id=\"T_983aa_row5_col2\" class=\"data row5 col2\" >N/A</td>\n",
       "      <td id=\"T_983aa_row5_col3\" class=\"data row5 col3\" >0.588200</td>\n",
       "      <td id=\"T_983aa_row5_col4\" class=\"data row5 col4\" >0.458300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x307d0be50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import joblib  # for saving sklearn models\n",
    "\n",
    "# Directory to save models\n",
    "save_dir = \"/Users/chetankothari/VS Projects/Threat_Analyzer_Autogen_Multiagent/classification_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ─── 6) Train & evaluate classical + boosting + CPU-XGB models ──────────────────────\n",
    "models = {\n",
    "    \"SGD_Logistic\":     SGDClassifier(loss=\"log_loss\", max_iter=1000, tol=1e-3, random_state=42),\n",
    "    \"RandomForest\":     RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=200, random_state=42),\n",
    "    \"XGB_CPU\":          XGBClassifier(tree_method='hist',\n",
    "                                      predictor='cpu_predictor',\n",
    "                                      use_label_encoder=False,\n",
    "                                      eval_metric='mlogloss',\n",
    "                                      random_state=42),\n",
    "    \"LinearSVM\":        SVC(kernel=\"linear\", probability=True, random_state=42),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, clf in models.items():\n",
    "    cv_scores = cross_val_score(clf, X_tr, y_tr, cv=5, scoring=\"f1_weighted\")\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    preds = clf.predict(X_te)\n",
    "    acc   = accuracy_score(y_te, preds)\n",
    "    f1    = classification_report(y_te, preds, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV F1 Mean\": round(cv_scores.mean(), 4),\n",
    "        \"CV F1 Std\":  round(cv_scores.std(), 4),\n",
    "        \"Test Accuracy\": round(acc, 4),\n",
    "        \"Test Macro F1\": round(f1, 4),\n",
    "    })\n",
    "    # Save the trained sklearn model\n",
    "    joblib.dump(clf, os.path.join(save_dir, f\"{name}.joblib\"))\n",
    "\n",
    "# ─── 7) Train & evaluate MLPClassifier ──────────────────────────────────────────\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_ds   = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_tr, dtype=torch.float32),\n",
    "    torch.tensor(y_tr, dtype=torch.long)\n",
    ")\n",
    "test_ds    = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_te, dtype=torch.float32),\n",
    "    torch.tensor(y_te, dtype=torch.long)\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "mlp      = MLPClassifier(input_dim=embs.shape[1], hidden_dim=2048, output_dim=len(np.unique(labels)))\n",
    "mlp.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(200):\n",
    "    mlp.train()\n",
    "    total_loss = 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(mlp(Xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/200 — Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save MLP state dict\n",
    "torch.save(mlp.state_dict(), os.path.join(save_dir, \"MLPClassifier.pth\"))\n",
    "\n",
    "mlp.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        preds = mlp(Xb.to(device)).argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "acc  = accuracy_score(all_labels, all_preds)\n",
    "f1_m = classification_report(all_labels, all_preds, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "results.append({\n",
    "    \"Model\":        \"MLPClassifier\",\n",
    "    \"CV F1 Mean\":  \"N/A\",\n",
    "    \"CV F1 Std\":   \"N/A\",\n",
    "    \"Test Accuracy\": round(acc, 4),\n",
    "    \"Test Macro F1\": round(f1_m, 4),\n",
    "})\n",
    "\n",
    "# ─── 8) Show final comparison ───────────────────────────────────────────────────\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.style.set_caption(\"Comparison of All Models (GBT & CPU-XGB + MLP)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test_new.json ===\n",
      "SGD_Logistic      -> Threat Level 3\n",
      "RandomForest      -> Threat Level 0\n",
      "GradientBoosting  -> Threat Level 0\n",
      "XGB_CPU           -> Threat Level 0\n",
      "LinearSVM         -> Threat Level 0\n",
      "MLPClassifier     -> Threat Level 0\n",
      "----------------------------------------\n",
      "\n",
      "=== videoframe_0.json ===\n",
      "SGD_Logistic      -> Threat Level 3\n",
      "RandomForest      -> Threat Level 0\n",
      "GradientBoosting  -> Threat Level 0\n",
      "XGB_CPU           -> Threat Level 0\n",
      "LinearSVM         -> Threat Level 0\n",
      "MLPClassifier     -> Threat Level 3\n",
      "----------------------------------------\n",
      "\n",
      "=== videoframe_52051.json ===\n",
      "SGD_Logistic      -> Threat Level 3\n",
      "RandomForest      -> Threat Level 1\n",
      "GradientBoosting  -> Threat Level 0\n",
      "XGB_CPU           -> Threat Level 0\n",
      "LinearSVM         -> Threat Level 1\n",
      "MLPClassifier     -> Threat Level 1\n",
      "----------------------------------------\n",
      "\n",
      "=== videoframe_6565.json ===\n",
      "SGD_Logistic      -> Threat Level 3\n",
      "RandomForest      -> Threat Level 0\n",
      "GradientBoosting  -> Threat Level 4\n",
      "XGB_CPU           -> Threat Level 0\n",
      "LinearSVM         -> Threat Level 1\n",
      "MLPClassifier     -> Threat Level 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ─── 1) Configuration ───────────────────────────────────────────────────────────\n",
    "# Directory where you saved your trained models\n",
    "save_dir = \"/Users/chetankothari/VS Projects/Threat_Analyzer_Autogen_Multiagent/classification_models\"\n",
    "\n",
    "# Directory of your validation JSON responses\n",
    "validation_response_path = (\n",
    "    \"/Users/chetankothari/VS Projects/Threat_Analyzer_Autogen_Multiagent/Data/validation_responses\"\n",
    ")\n",
    "\n",
    "# ─── 2) Load BERT embedder ───────────────────────────────────────────────────────\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").eval()\n",
    "\n",
    "def extract_bert_embeddings(text: str) -> np.ndarray:\n",
    "    \"\"\"Returns a 768-dim vector for the input text via mean-pooling.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                       padding=\"max_length\", max_length=128)\n",
    "    with torch.no_grad():\n",
    "        out = bert_model(**inputs)\n",
    "    # (batch, seq, hidden) -> mean over seq -> (batch, hidden)\n",
    "    return out.last_hidden_state.mean(dim=1).cpu().numpy().squeeze()\n",
    "\n",
    "# ─── 3) Reload your classical models ─────────────────────────────────────────────\n",
    "models = {}\n",
    "# SGD “Logistic”\n",
    "models[\"SGD_Logistic\"] = joblib.load(os.path.join(save_dir, \"SGD_Logistic.joblib\"))\n",
    "# RandomForest\n",
    "models[\"RandomForest\"] = joblib.load(os.path.join(save_dir, \"RandomForest.joblib\"))\n",
    "# GradientBoosting\n",
    "models[\"GradientBoosting\"] = joblib.load(os.path.join(save_dir, \"GradientBoosting.joblib\"))\n",
    "# XGB_CPU\n",
    "models[\"XGB_CPU\"] = joblib.load(os.path.join(save_dir, \"XGB_CPU.joblib\"))\n",
    "# LinearSVM\n",
    "models[\"LinearSVM\"] = joblib.load(os.path.join(save_dir, \"LinearSVM.joblib\"))\n",
    "\n",
    "# ─ 4) Reload your MLP (PyTorch) ────────────────────────────────────────────────\n",
    "class MLPClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# You must know input_dim (768), hidden_dim (2048) and output_dim (#classes)\n",
    "INPUT_DIM  = 768\n",
    "HIDDEN_DIM = 2048\n",
    "OUTPUT_DIM = 5  # adjust if you have 0–4 => 5 classes\n",
    "\n",
    "mlp = MLPClassifier(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "mlp.load_state_dict(torch.load(os.path.join(save_dir, \"MLPClassifier.pth\")))\n",
    "mlp.eval()\n",
    "\n",
    "# ─── 5) Inference loop ───────────────────────────────────────────────────────────\n",
    "for fname in sorted(os.listdir(validation_response_path)):\n",
    "    path = os.path.join(validation_response_path, fname)\n",
    "    if not fname.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    # Load JSON\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract vision_analysis (dict or string), stringify\n",
    "    vis = data.get(\"vision_analysis\", \"\")\n",
    "    vis_text = json.dumps(vis) if isinstance(vis, dict) else str(vis)\n",
    "\n",
    "    # Embed\n",
    "    emb = extract_bert_embeddings(vis_text).reshape(1, -1)\n",
    "\n",
    "    # Prepare for MLP\n",
    "    emb_tensor = torch.tensor(emb, dtype=torch.float32)\n",
    "\n",
    "    # Predict with classical models\n",
    "    preds = {name: int(clf.predict(emb)[0]) for name, clf in models.items()}\n",
    "\n",
    "    # Predict with MLP\n",
    "    with torch.no_grad():\n",
    "        mlp_out = mlp(emb_tensor)\n",
    "        preds[\"MLPClassifier\"] = int(mlp_out.argmax(dim=1).item())\n",
    "\n",
    "    # Display\n",
    "    print(f\"\\n=== {fname} ===\")\n",
    "    for model_name, threat in preds.items():\n",
    "        print(f\"{model_name:17s} -> Threat Level {threat}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
